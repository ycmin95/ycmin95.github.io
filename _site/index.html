<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Yuecong Min (ÈóµË∂äËÅ™) - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Yuecong Min (ÈóµË∂äËÅ™)">
<meta property="og:title" content="Yuecong Min (ÈóµË∂äËÅ™)">


  <link rel="canonical" href="https://github.com/pages/ycmin95/ycmin95.github.io/">
  <meta property="og:url" content="https://github.com/pages/ycmin95/ycmin95.github.io/">



  <meta property="og:description" content="Deep Learning, Deeper Thinking.">









<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/#about-me">About Me</a></li>
          
            <li class="masthead__menu-item"><a href="/#-news">News</a></li>
          
            <li class="masthead__menu-item"><a href="/#-publications">Publications</a></li>
          
            <li class="masthead__menu-item"><a href="/#-honors-and-awards">Honors and Awards</a></li>
          
            <li class="masthead__menu-item"><a href="/#-educations">Educations</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="images/profile2.jpg" class="author__avatar" alt="Yuecong Min">
  </div>

  <div class="author__content">
    <h3 class="author__name">Yuecong Min</h3>
    <p class="author__bio">Institute of Computing Technology, Chinese Academy of Sciences</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;">Deep Learning, Deeper Thinking.</div></li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Beijing, China</li>
      
      
      
      
        <li><a href="mailto:minyuecong17@mails.ucas.ac.cn"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/ycmin95"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=qc2906sAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="mailto:minyuecong17@mails.ucas.ac.cn"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
      
      
      
      
      
      
      
      
      
        <a href="https://github.com/ycmin95"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com/citations?user=qc2906sAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            
<p><span class="anchor" id="about-me"></span></p>

<p>I am currently a Ph.D. candidate in Visual Information Processing and Learning (<a href="https://vipl.ict.ac.cn/">VIPL</a>) group from the Institute of Computing Technology, Chinese Academy of Sciences, under the supervision of Prof. <a href="https://scholar.google.com/citations?user=vVx2v20AAAAJ&amp;hl=en&amp;oi=ao">Xilin Chen</a>. I also had close collaboration with Prof. <a href="https://scholar.google.com/citations?user=ZcL91HsAAAAJ&amp;hl=en&amp;oi=ao">Xiujuan Chai</a>. My research interests mainly focus on human behavior analysis and understanding from sequential data, especially for gesture and sign language. I am also interested in exploring how to use data in deep learning more efficient. Before that, I obtained my bachelor degree from Shandong University.</p>

<!-- During my time pursuing a Ph.D., I focus on efficient visual sequence recognition algorithm designs. For short-term recognition tasks 
(including gesture and isolated sign language recognition), we propose several approaches to adopt the sparse point cloud sequence sampled from 
the depth video for recognition. Compared to video-based algorithms, the point-cloud-based algorithm is more efficient and more sensitive to distance changes, 
and shows effectiveness in gesture recognition and action recognition tasks. After that, I am interested in long-term recognition (i.e., Continuous Sign Language Recognition (CSLR)) 
and find that current approaches are inefficient. Training a video-based CSLR model needs more than 70 hours, which is unbearable :( !!! After several attempts, we attribute the 
overfitting of the powerful alignment module to the major problem of the CTC-based CSLR optimization, which leads to insufficient training of the feature extractor. Several 
constraints are proposed to make the training process more efficient and are adopted by recent CSLR works. I am trying to provide more insights into this phenomenon. -->

<!-- I am looking for a suitable job (vision-based or sequence-based). I believe the job hunting is a two-way selection process. 
If there is a suitable position, please feel free to contact me :)

My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=qc2906sAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=qc2906sAAAAJ'><img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2Fycmin95%2Fycmin95.github.io%40google-scholar-stats%2Fgs_data_shieldsio.json&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->

<h1 id="-news">üî• News</h1>
<ul>
  <li><em>2022.07</em>: ¬†üéâüéâ One paper is accepted by ECCV 2022!</li>
</ul>

<h1 id="-publications">üìù Publications</h1>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ECCV 2022</div><img src="images/radialCTC.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5670_ECCV_2022_paper.php">Deep Radial Embedding for Visual Sequence Learning</a></p>

    <p><strong>Yuecong Min</strong>, Peiqi Jiao, Yanan Li, Xiaotao Wang, Lei Lei, Xiujuan Chai, Xilin Chen</p>

    <!-- [**Project**](https://github.com/ycmin95/VAC_CSLR) <strong><span class='show_paper_citations' data='qc2906sAAAAJ:u_35RYKgDlwC'></span></strong> -->
    <ul>
      <li>RadialCTC constrains sequence features on a hypersphere while retaining the iterative alignment mechanism of CTC, which also provides a clear geometric interpretation for CTC</li>
      <li>RadialCTC controls the peaky behavior with a simple angular perturbation term</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ICCV 2021</div><img src="images/vac.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://openaccess.thecvf.com/content/ICCV2021/html/Min_Visual_Alignment_Constraint_for_Continuous_Sign_Language_Recognition_ICCV_2021_paper.html">Visual Alignment Constraint for Continuous Sign Language Recognition</a></p>

    <p><strong>Yuecong Min</strong>, Aiming Hao, Xiujuan Chai, Xilin Chen</p>

    <!-- [**Project**](https://github.com/ycmin95/VAC_CSLR) <strong><span class='show_paper_citations' data='qc2906sAAAAJ:zA6iFVUQeVQC'></span></strong> \|  -->
    <p><a href="https://github.com/ycmin95/VAC_CSLR"><img src="https://img.shields.io/github/stars/ycmin95/VAC_CSLR?style=social&amp;label=VAC Stars" alt="" /></a> <a href="https://github.com/ycmin95/VAC_CSLR"><img src="https://img.shields.io/github/forks/ycmin95/VAC_CSLR?style=social&amp;label=Forks" alt="" /></a></p>
    <ul>
      <li>VAC provides an efficient way to make CSLR models end-to-end trainable and is adopted as the baseline model by many recent works</li>
      <li>Two metrics to evaluate the contributions of the feature extractor and the alignment module</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">CVPR 2020</div><img src="images/pointlstm.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Min_An_Efficient_PointLSTM_for_Point_Clouds_Based_Gesture_Recognition_CVPR_2020_paper.html">An Efficient PointLSTM for Point Clouds Based Gesture Recognition</a></p>

    <p><strong>Yuecong Min</strong>, Yanxiao Zhang, Xiujuan Chai, Xilin Chen</p>

    <p><a href="https://github.com/ycmin95/pointlstm-gesture-recognition-pytorch"><img src="https://img.shields.io/github/stars/ycmin95/pointlstm-gesture-recognition-pytorch?style=social&amp;label=PointLSTM Stars" alt="" /></a> <a href="https://github.com/ycmin95/pointlstm-gesture-recognition-pytorch"><img src="https://img.shields.io/github/forks/ycmin95/pointlstm-gesture-recognition-pytorch?style=social&amp;label=Forks" alt="" /></a></p>
    <ul>
      <li>PointLSTM can leverage long-term spatio-temporal relationships in irregular sequence data (e.g., point cloud) while preserving the spatial structure for irregular sequence recognition problem</li>
      <li>Evaluation results on 3D gesture recognition and action recognition show great potential for real-time applications</li>
    </ul>
  </div>
</div>

<ul>
  <li><a href="https://openaccess.thecvf.com/content/ICCV2021/html/Hao_Self-Mutual_Distillation_Learning_for_Continuous_Sign_Language_Recognition_ICCV_2021_paper.html">Self-Mutual Distillation Learning for Continuous Sign Language Recognition.</a> Aiming Hao, <strong>Yuecong Min</strong>, and Xilin Chen, International Conference on Computer Vision (ICCV), 2021.</li>
  <li><a href="https://www.sciencedirect.com/science/article/pii/S2096579621000309">Teaching Chinese Sign Language with A Smartphone.</a> Yanxiao Zhang, <strong>Yuecong Min</strong>, Xilin Chen, Virtual Reality &amp; Intelligent Hardware, 2021.</li>
  <li><a href="https://bmvc2019.org/wp-content/uploads/papers/0326-paper.pdf">FlickerNet: Adaptive 3D Gesture Recognition from Sparse Point Clouds.</a> <strong>Yuecong Min</strong>, Xiujuan Chai, Lei Zhao, Xilin Chen, British Machine Vision Conference (BMVC), 2019.</li>
</ul>

<h1 id="-honors-and-awards">üéñ Honors and Awards</h1>
<ul>
  <li><strong>China National Scholarship for Ph.D.</strong>, ICT, CAS, 2020</li>
</ul>

<h1 id="-educations">üìñ Educations</h1>
<ul>
  <li>2017.09 - now, I am a Ph.D. student at Institute of Computing Technology, CAS, under the supervision of Prof. <a href="https://scholar.google.com/citations?user=vVx2v20AAAAJ&amp;hl=en&amp;oi=ao">Xilin Chen</a>.</li>
  <li>2013.09 - 2017.07, I was a college student in Shandong University, Weihai.</li>
</ul>

<h1 id="Ô∏è-academic-services">‚úíÔ∏è Academic Services</h1>
<ul>
  <li>Invited journal reviewer for
IEEE TPAMI / IEEE TMM / IEEE TIP / PR ‚Ä¶</li>
  <li>Invited conference reviewer for
CVPR‚Äô22 /ACM MM‚Äô22 / ECCV‚Äô22 / CVPR‚Äô23 ‚Ä¶</li>
</ul>

<h1 id="Ô∏è-misc">‚öôÔ∏è Misc</h1>
<ul>
  <li>A summary of papers on gesture and sign language recognition. <a href="https://github.com/ycmin95/awesome-Gesture-Sign-Language-Recognition"><img src="https://img.shields.io/github/stars/ycmin95/awesome-Gesture-Sign-Language-Recognition?style=social&amp;label=VAC Stars" alt="" /></a></li>
  <li>A simple tool to visualize the main keywords of accepted papers for the recent Computer Vision conferences <a href="https://github.com/ycmin95/CVPaperStatistics"><img src="https://img.shields.io/github/stars/ycmin95/CVPaperStatistics?style=social&amp;label=VAC Stars" alt="" /></a></li>
</ul>

<!-- # üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->

          </section>
        </div>
      </article>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://cdn.jsdelivr.net/gh/ycmin95/ycmin95.github.io@'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            var totalCitation = data['citedby']
            document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>


  </body>
</html>
